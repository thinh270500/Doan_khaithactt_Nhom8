import os
from sklearn.feature_extraction.text import TfidfVectorizer
from gensim.models import Word2Vec
import numpy as np
from nltk.tokenize import word_tokenize
import nltk
import re
nltk.download('punkt')


# 1. LÃ m sáº¡ch vÄƒn báº£n
def clean_text(text):
    text = text.lower()
    text = re.sub(r'\d+', '', text)  # bá» sá»‘
    text = re.sub(r'[^\w\s]', '', text)  # bá» dáº¥u cÃ¢u
    text = re.sub(r'\s+', ' ', text).strip()  # bá» khoáº£ng tráº¯ng thá»«a
    return text

# ----------------------------------
# 2. Äá»c dá»¯ liá»‡u vÃ  tiá»n xá»­ lÃ½
with open("data/cleaned_articles.data", "r", encoding="utf-8") as f:
    raw_lines = [line.strip() for line in f if line.strip()]

corpus = [clean_text(line) for line in raw_lines]

print(f"âœ… Sá»‘ vÄƒn báº£n sau khi lÃ m sáº¡ch: {len(corpus)}")

# ----------------------------------
# 3. TF-IDF
vietnamese_stopwords = [
    'vÃ ', 'lÃ ', 'cá»§a', 'cÃ³', 'cho', 'vá»›i', 'nhá»¯ng', 'cÃ¡c', 'Ä‘Æ°á»£c',
    'trÃªn', 'táº¡i', 'má»™t', 'nÃ y', 'Ä‘Ã£', 'ráº±ng', 'thÃ¬', 'láº¡i', 'sáº½',
    'khi', 'Ä‘áº¿n', 'Ä‘i', 'á»Ÿ', 'vá»', 'Ä‘Ã³', 'nÃªn', 'vÃ¬', 'náº¿u', 'tÃ´i',
    'báº¡n', 'chÃºng', 'tÃ´i', 'anh', 'chá»‹', 'nÃ³', 'há»', 'váº«n', 'Ä‘ang'
]

vectorizer = TfidfVectorizer(
    stop_words=vietnamese_stopwords,
    token_pattern=r'(?u)\b[^\d\W]+\b'  # chá»‰ láº¥y tá»« chá»©a chá»¯ cÃ¡i
)

tfidf_matrix = vectorizer.fit_transform(corpus)
print(f"ğŸ”¹ TF-IDF shape: {tfidf_matrix.shape}")
print("ğŸ”¹ Má»™t sá»‘ tá»« khÃ³a TF-IDF:", vectorizer.get_feature_names_out()[:10])

# ----------------------------------
# 4. Huáº¥n luyá»‡n Word2Vec vá»›i underthesea

# TÃ¡ch tá»« cho má»—i vÄƒn báº£n
tokenized_corpus = [word_tokenize(doc) for doc in corpus]

# Huáº¥n luyá»‡n Word2Vec
w2v_model = Word2Vec(
    sentences=tokenized_corpus,
    vector_size=100,
    window=5,
    min_count=1,
    workers=4,
    sg=1,  # dÃ¹ng Skip-Gram
    compute_loss=True
)
w2v_model.build_vocab(tokenized_corpus)

loss_values = []
previous_loss = 0.0
epochs = 10

for epoch in range(1, epochs + 1):
    w2v_model.train(
        tokenized_corpus,
        total_examples=w2v_model.corpus_count,
        epochs=1,
        compute_loss=True
    )
    current_loss = w2v_model.get_latest_training_loss()
    epoch_loss = current_loss - previous_loss  # tÃ­nh chÃªnh lá»‡ch
    loss_values.append(epoch_loss)
    previous_loss = current_loss
    print(f"ğŸ” Epoch {epoch}: Loss = {epoch_loss:.2f}")
# Kiá»ƒm tra vector cá»§a má»™t tá»«
sample_word = "khÃ¡ch"
if sample_word in w2v_model.wv:
    print(f"ğŸ”¹ Vector tá»« '{sample_word}':\n", w2v_model.wv[sample_word])
else:
    print(f"âš ï¸ Tá»« '{sample_word}' khÃ´ng cÃ³ trong tá»« Ä‘iá»ƒn Word2Vec.")

# ----------------------------------
# 5. Biá»ƒu diá»…n vÄƒn báº£n báº±ng trung bÃ¬nh vector tá»«

def document_vector(doc):
    words = word_tokenize(doc)
    vectors = [w2v_model.wv[word] for word in words if word in w2v_model.wv]
    return np.mean(vectors, axis=0) if vectors else np.zeros(w2v_model.vector_size)

# Biá»ƒu diá»…n toÃ n bá»™ vÄƒn báº£n
doc_vectors = [document_vector(doc) for doc in corpus]

# Kiá»ƒm tra
print(f"\nğŸ”¸ Vector trung bÃ¬nh cho vÄƒn báº£n 1:\n{doc_vectors[0]}")
print(f"âœ… KÃ­ch thÆ°á»›c vector má»—i vÄƒn báº£n: {doc_vectors[0].shape}")

# GensimWord2Vec_model = Word2Vec(corpus,
#                                 vector_size=100,
#                                 min_count=1,  # sá»‘ láº§n xuáº¥t hiá»‡n tháº¥p nháº¥t cá»§a má»—i tá»« vá»±ng
#                                 window=2,  # khai bÃ¡o kÃ­ch thÆ°á»›c windows size
#                                 sg=8,  # sg = 1 sá»­ dá»¥ng mÃ´ hÃ¬nh skip-grams - sg=0 -> sá»­ dá»¥ng CBOW
#                                 workers=1
#                                 )

# print('TÃ¬m top-10 tá»« tÆ°Æ¡ng Ä‘á»“ng vá»›i tá»«: [khÃ¡ch]')
# # for index, word_tuple in enumerate(GensimWord2Vec_model.wv.similar_by_word("khÃ¡ch")):
# #     print('%s.%s\t\t%s' % (index, word_tuple[0], word_tuple[1]))
# word = "khÃ¡ch"

# if word in GensimWord2Vec_model.wv:
#     for index, (similar_word, similarity) in enumerate(GensimWord2Vec_model.wv.similar_by_word(word), start=1):
#         print(f"{index}. {similar_word}\t\t{similarity:.4f}")
# else:
#     print(f"âš ï¸ Tá»« '{word}' khÃ´ng cÃ³ trong tá»« Ä‘iá»ƒn cá»§a mÃ´ hÃ¬nh.")
print("\nğŸ“‰ Tá»•ng quan loss theo tá»«ng epoch:")
for i, loss in enumerate(loss_values, 1):
    print(f"Epoch {i}: Loss = {loss:.2f}")
print("\nğŸ“Œ Top tá»« tÆ°Æ¡ng Ä‘á»“ng vá»›i 'khÃ¡ch':")
if "khÃ¡ch" in w2v_model.wv:
    for index, (similar_word, similarity) in enumerate(w2v_model.wv.similar_by_word("khÃ¡ch", topn=10), start=1):
        print(f"{index}. {similar_word}\t\t{similarity:.4f}")
else:
    print(f"âš ï¸ Tá»« 'khÃ¡ch' khÃ´ng cÃ³ trong tá»« Ä‘iá»ƒn.")